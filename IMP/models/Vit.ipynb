{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c797d50a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch version: 2.1.1\n",
      "torchvision version: 0.16.1\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# For this notebook to run with updated APIs, we need torch 1.12+ and torchvision 0.13+\n",
    "import torch\n",
    "import torchvision\n",
    "print(f\"torch version: {torch.__version__}\")\n",
    "print(f\"torchvision version: {torchvision.__version__}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0194cad3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda:0\n"
     ]
    }
   ],
   "source": [
    "from copy import deepcopy\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from PIL import Image\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score, recall_score, f1_score\n",
    "import seaborn as sns\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import models, transforms, utils\n",
    "import torch.optim as optim\n",
    "from torch.optim import lr_scheduler\n",
    "from torchinfo import summary\n",
    "\n",
    "import time\n",
    "import os\n",
    "import zipfile\n",
    "from copy import deepcopy\n",
    "\n",
    "import import_ipynb\n",
    "\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)\n",
    "\n",
    "# vision transformer\n",
    "#from linformer-pytorch import Linformer\n",
    "#from vit_pytorch.efficient import ViT\n",
    "\n",
    "from vit_pytorch.model import ViT\n",
    "from linformer_pytorch import Linformer\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "49ae6f73",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'cuda'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "\n",
    "# Setup device agnostic code\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "device\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "936ff2ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "    \n",
    "input_size=224\n",
    "\n",
    "\n",
    "train_transformations= transforms.Compose([transforms.CenterCrop(input_size),\n",
    "                                     transforms.RandomRotation(90),\n",
    "                                     transforms.RandomHorizontalFlip(),\n",
    "                                     transforms.RandomVerticalFlip(),\n",
    "                                      transforms.ToTensor(),\n",
    "                                      transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "                                     ])\n",
    "\n",
    "\n",
    "\n",
    "valid_transformations= transforms.Compose([transforms.CenterCrop(input_size),\n",
    "                                          transforms.ToTensor(),\n",
    "                                          transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])])\n",
    "\n",
    "\n",
    "\n",
    "test_transformations= transforms.Compose([transforms.CenterCrop(input_size),\n",
    "                                         transforms.ToTensor(),\n",
    "                                         transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])])\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f6845a69",
   "metadata": {},
   "outputs": [],
   "source": [
    "NUM_WORKERS = os.cpu_count()\n",
    "\n",
    "from torchvision import datasets, transforms\n",
    "from torch.utils.data import DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "07e4bd2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_train_dataloader(train_dir: str,\n",
    "                            transform: transforms.Compose,\n",
    "                            batch_size: int,\n",
    "                            num_workers: int=NUM_WORKERS\n",
    "                           ):\n",
    "    train_data= datasets.ImageFolder(train_dir,transform=transform)\n",
    "    \n",
    "    \n",
    "    classes= train_data.classes\n",
    "    \n",
    "    \n",
    "    train_dataloader = DataLoader(\n",
    "        \n",
    "        train_data,\n",
    "        batch_size=batch_size,\n",
    "        shuffle=True,\n",
    "        num_workers=num_workers,\n",
    "        pin_memory=True,\n",
    "        \n",
    "     )\n",
    "   \n",
    "    \n",
    "    return train_dataloader , classes\n",
    "    \n",
    "     \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "cf15a1da",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_validate_dataloader(validation_dir: str,\n",
    "                            transform: transforms.Compose,\n",
    "                            batch_size: int,\n",
    "                            num_workers: int=NUM_WORKERS\n",
    "                           ):\n",
    "    validation_data= datasets.ImageFolder(validation_dir,transform=transform)\n",
    "    #classes= validation_data.classes\n",
    "\n",
    "    validation_dataloader = DataLoader(\n",
    "        validation_data,\n",
    "        batch_size=batch_size,\n",
    "        shuffle=True,\n",
    "        num_workers=num_workers,\n",
    "        pin_memory=True,\n",
    "        \n",
    "     )\n",
    "    \n",
    "    return validation_dataloader #classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "61931677",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_test_dataloader(testtrain_dir: str,\n",
    "                            transform: transforms.Compose,\n",
    "                            batch_size: int,\n",
    "                            num_workers: int=NUM_WORKERS\n",
    "                           ):\n",
    "    test_data= datasets.ImageFolder(test_dir,transform=transform)\n",
    "    test_dataloader = DataLoader(\n",
    "        \n",
    "        test_data,\n",
    "        batch_size=batch_size,\n",
    "        shuffle=True,\n",
    "        num_workers=num_workers,\n",
    "        pin_memory=True,\n",
    "        \n",
    "     )\n",
    "    \n",
    "    return test_dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "acef3fe6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import zipfile\n",
    "\n",
    "from pathlib import Path\n",
    "\n",
    "import requests\n",
    "\n",
    "# Setup path to data folder\n",
    "path = \"C:/Users/Mohamme/Desktop/THESIS STUFF/data\"\n",
    "data_path = Path(path)\n",
    "image_path = data_path / \"data\"\n",
    "\n",
    "# If the image folder doesn't exist, download it and prepare it... \n",
    "if image_path.is_dir():\n",
    "    print(f\"{image_path} directory exists.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "039135fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup Dirs\n",
    "train_dir = image_path / \"C:/Users/Mohamme/Desktop/THESIS STUFF/data/TRAIN\"\n",
    "validation_dir = image_path / \"C:/Users/Mohamme/Desktop/THESIS STUFF/data/TEST\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "1b3d844a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(<torch.utils.data.dataloader.DataLoader at 0x1bb921c2190>,\n",
       " <torch.utils.data.dataloader.DataLoader at 0x1bb91df54d0>,\n",
       " ['Barred Spiral',\n",
       "  'Cigar Shaped Smooth',\n",
       "  'Disturbed',\n",
       "  'Edge-on with Bulge',\n",
       "  'Edge-on without Bulge',\n",
       "  'Merging',\n",
       "  'Round Smooth',\n",
       "  'Smooth, Cigar shaped',\n",
       "  'Unbarred Loose Spiral',\n",
       "  'Unbarred Tight Spiral'])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Create Transformers\n",
    "#train_transformations, valid_transformations, test_transformations = data_transformation()\n",
    "\n",
    "#create Datasets\n",
    "train_data,  class_names = create_train_dataloader(train_dir=train_dir,transform=train_transformations, batch_size=32)\n",
    "validation_data= create_validate_dataloader(validation_dir=validation_dir,transform=valid_transformations,batch_size=32)\n",
    "\n",
    "train_data, validation_data , class_names\n",
    "\n",
    "\n",
    "\n",
    "#ADD THE TEST LATER!!!!!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "b1a6ca14",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import os\n",
    "import zipfile\n",
    "\n",
    "from pathlib import Path\n",
    "\n",
    "import requests\n",
    "\n",
    "# Setup path to data folder\n",
    "path = \"C:/Users/Mohamme/Desktop/THESIS STUFF/data\"\n",
    "data_path = Path(path)\n",
    "image_path = data_path / \"data\"\n",
    "\n",
    "# If the image folder doesn't exist, download it and prepare it... \n",
    "if image_path.is_dir():\n",
    "    print(f\"{image_path} directory exists.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "f14ba062",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup Dirs\n",
    "train_dir = image_path / \"C:/Users/Mohamme/Desktop/THESIS STUFF/data/TRAIN\"\n",
    "test_dir = image_path / \"C:/Users/Mohamme/Desktop/THESIS STUFF/data/TEST\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "bad679c3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ViT_B_16_Weights.IMAGENET1K_V1"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torchvision\n",
    "# Get a set of pretrained model weights\n",
    "weights = torchvision.models.ViT_B_16_Weights.IMAGENET1K_V1 # .DEFAULT = best available weights from pretraining on ImageNet\n",
    "weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "eeeb0f35",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ImageClassification(\n",
       "    crop_size=[224]\n",
       "    resize_size=[256]\n",
       "    mean=[0.485, 0.456, 0.406]\n",
       "    std=[0.229, 0.224, 0.225]\n",
       "    interpolation=InterpolationMode.BILINEAR\n",
       ")"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "# Get the transforms used to create our pretrained weights\n",
    "auto_transforms = weights.transforms()\n",
    "auto_transforms\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc6ad49e",
   "metadata": {},
   "source": [
    "# Create Datasets and DataLoaders\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9d7338e",
   "metadata": {},
   "source": [
    "\n",
    "# Creating a transform for torchvision.models\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "5396e690",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ViT_B_16_Weights.IMAGENET1K_V1"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Get a set of pretrained model weights\n",
    "weights = torchvision.models.ViT_B_16_Weights.IMAGENET1K_V1 # .DEFAULT = best available weights from pretraining on ImageNet\n",
    "weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "8a1d7b1d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "VisionTransformer(\n",
       "  (conv_proj): Conv2d(3, 768, kernel_size=(16, 16), stride=(16, 16))\n",
       "  (encoder): Encoder(\n",
       "    (dropout): Dropout(p=0.0, inplace=False)\n",
       "    (layers): Sequential(\n",
       "      (encoder_layer_0): EncoderBlock(\n",
       "        (ln_1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "        (self_attention): MultiheadAttention(\n",
       "          (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
       "        )\n",
       "        (dropout): Dropout(p=0.0, inplace=False)\n",
       "        (ln_2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "        (mlp): MLPBlock(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU(approximate='none')\n",
       "          (2): Dropout(p=0.0, inplace=False)\n",
       "          (3): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (4): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (encoder_layer_1): EncoderBlock(\n",
       "        (ln_1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "        (self_attention): MultiheadAttention(\n",
       "          (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
       "        )\n",
       "        (dropout): Dropout(p=0.0, inplace=False)\n",
       "        (ln_2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "        (mlp): MLPBlock(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU(approximate='none')\n",
       "          (2): Dropout(p=0.0, inplace=False)\n",
       "          (3): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (4): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (encoder_layer_2): EncoderBlock(\n",
       "        (ln_1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "        (self_attention): MultiheadAttention(\n",
       "          (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
       "        )\n",
       "        (dropout): Dropout(p=0.0, inplace=False)\n",
       "        (ln_2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "        (mlp): MLPBlock(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU(approximate='none')\n",
       "          (2): Dropout(p=0.0, inplace=False)\n",
       "          (3): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (4): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (encoder_layer_3): EncoderBlock(\n",
       "        (ln_1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "        (self_attention): MultiheadAttention(\n",
       "          (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
       "        )\n",
       "        (dropout): Dropout(p=0.0, inplace=False)\n",
       "        (ln_2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "        (mlp): MLPBlock(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU(approximate='none')\n",
       "          (2): Dropout(p=0.0, inplace=False)\n",
       "          (3): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (4): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (encoder_layer_4): EncoderBlock(\n",
       "        (ln_1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "        (self_attention): MultiheadAttention(\n",
       "          (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
       "        )\n",
       "        (dropout): Dropout(p=0.0, inplace=False)\n",
       "        (ln_2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "        (mlp): MLPBlock(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU(approximate='none')\n",
       "          (2): Dropout(p=0.0, inplace=False)\n",
       "          (3): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (4): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (encoder_layer_5): EncoderBlock(\n",
       "        (ln_1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "        (self_attention): MultiheadAttention(\n",
       "          (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
       "        )\n",
       "        (dropout): Dropout(p=0.0, inplace=False)\n",
       "        (ln_2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "        (mlp): MLPBlock(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU(approximate='none')\n",
       "          (2): Dropout(p=0.0, inplace=False)\n",
       "          (3): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (4): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (encoder_layer_6): EncoderBlock(\n",
       "        (ln_1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "        (self_attention): MultiheadAttention(\n",
       "          (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
       "        )\n",
       "        (dropout): Dropout(p=0.0, inplace=False)\n",
       "        (ln_2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "        (mlp): MLPBlock(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU(approximate='none')\n",
       "          (2): Dropout(p=0.0, inplace=False)\n",
       "          (3): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (4): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (encoder_layer_7): EncoderBlock(\n",
       "        (ln_1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "        (self_attention): MultiheadAttention(\n",
       "          (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
       "        )\n",
       "        (dropout): Dropout(p=0.0, inplace=False)\n",
       "        (ln_2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "        (mlp): MLPBlock(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU(approximate='none')\n",
       "          (2): Dropout(p=0.0, inplace=False)\n",
       "          (3): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (4): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (encoder_layer_8): EncoderBlock(\n",
       "        (ln_1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "        (self_attention): MultiheadAttention(\n",
       "          (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
       "        )\n",
       "        (dropout): Dropout(p=0.0, inplace=False)\n",
       "        (ln_2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "        (mlp): MLPBlock(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU(approximate='none')\n",
       "          (2): Dropout(p=0.0, inplace=False)\n",
       "          (3): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (4): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (encoder_layer_9): EncoderBlock(\n",
       "        (ln_1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "        (self_attention): MultiheadAttention(\n",
       "          (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
       "        )\n",
       "        (dropout): Dropout(p=0.0, inplace=False)\n",
       "        (ln_2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "        (mlp): MLPBlock(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU(approximate='none')\n",
       "          (2): Dropout(p=0.0, inplace=False)\n",
       "          (3): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (4): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (encoder_layer_10): EncoderBlock(\n",
       "        (ln_1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "        (self_attention): MultiheadAttention(\n",
       "          (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
       "        )\n",
       "        (dropout): Dropout(p=0.0, inplace=False)\n",
       "        (ln_2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "        (mlp): MLPBlock(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU(approximate='none')\n",
       "          (2): Dropout(p=0.0, inplace=False)\n",
       "          (3): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (4): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (encoder_layer_11): EncoderBlock(\n",
       "        (ln_1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "        (self_attention): MultiheadAttention(\n",
       "          (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
       "        )\n",
       "        (dropout): Dropout(p=0.0, inplace=False)\n",
       "        (ln_2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "        (mlp): MLPBlock(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU(approximate='none')\n",
       "          (2): Dropout(p=0.0, inplace=False)\n",
       "          (3): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (4): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (ln): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "  )\n",
       "  (heads): Sequential(\n",
       "    (head): Linear(in_features=768, out_features=1000, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# NEW: Setup the model with pretrained weights and send it to the target device (torchvision v0.13+)\n",
    "weights = torchvision.models.ViT_B_16_Weights.IMAGENET1K_V1\n",
    "model = torchvision.models.vit_b_16(weights = weights, progress = False).to(device)\n",
    "\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "cf9a86f3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ImageClassification(\n",
       "    crop_size=[224]\n",
       "    resize_size=[256]\n",
       "    mean=[0.485, 0.456, 0.406]\n",
       "    std=[0.229, 0.224, 0.225]\n",
       "    interpolation=InterpolationMode.BILINEAR\n",
       ")"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "\n",
    "# Get the transforms used to create our pretrained weights\n",
    "auto_transforms = weights.transforms()\n",
    "auto_transforms\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "53b1722b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "====================================================================================================\n",
       "Layer (type (var_name))                                      Input Shape Output Shape Param #    Trainable\n",
       "====================================================================================================\n",
       "VisionTransformer (VisionTransformer)                        [32, 3, 224, 224] [32, 1000] 768        True\n",
       "├─Conv2d (conv_proj)                                         [32, 3, 224, 224] [32, 768, 14, 14] 590,592    True\n",
       "├─Encoder (encoder)                                          [32, 197, 768] [32, 197, 768] 151,296    True\n",
       "│    └─Dropout (dropout)                                     [32, 197, 768] [32, 197, 768] --         --\n",
       "│    └─Sequential (layers)                                   [32, 197, 768] [32, 197, 768] --         True\n",
       "│    │    └─EncoderBlock (encoder_layer_0)                   [32, 197, 768] [32, 197, 768] 7,087,872  True\n",
       "│    │    └─EncoderBlock (encoder_layer_1)                   [32, 197, 768] [32, 197, 768] 7,087,872  True\n",
       "│    │    └─EncoderBlock (encoder_layer_2)                   [32, 197, 768] [32, 197, 768] 7,087,872  True\n",
       "│    │    └─EncoderBlock (encoder_layer_3)                   [32, 197, 768] [32, 197, 768] 7,087,872  True\n",
       "│    │    └─EncoderBlock (encoder_layer_4)                   [32, 197, 768] [32, 197, 768] 7,087,872  True\n",
       "│    │    └─EncoderBlock (encoder_layer_5)                   [32, 197, 768] [32, 197, 768] 7,087,872  True\n",
       "│    │    └─EncoderBlock (encoder_layer_6)                   [32, 197, 768] [32, 197, 768] 7,087,872  True\n",
       "│    │    └─EncoderBlock (encoder_layer_7)                   [32, 197, 768] [32, 197, 768] 7,087,872  True\n",
       "│    │    └─EncoderBlock (encoder_layer_8)                   [32, 197, 768] [32, 197, 768] 7,087,872  True\n",
       "│    │    └─EncoderBlock (encoder_layer_9)                   [32, 197, 768] [32, 197, 768] 7,087,872  True\n",
       "│    │    └─EncoderBlock (encoder_layer_10)                  [32, 197, 768] [32, 197, 768] 7,087,872  True\n",
       "│    │    └─EncoderBlock (encoder_layer_11)                  [32, 197, 768] [32, 197, 768] 7,087,872  True\n",
       "│    └─LayerNorm (ln)                                        [32, 197, 768] [32, 197, 768] 1,536      True\n",
       "├─Sequential (heads)                                         [32, 768]  [32, 1000] --         True\n",
       "│    └─Linear (head)                                         [32, 768]  [32, 1000] 769,000    True\n",
       "====================================================================================================\n",
       "Total params: 86,567,656\n",
       "Trainable params: 86,567,656\n",
       "Non-trainable params: 0\n",
       "Total mult-adds (Units.GIGABYTES): 5.54\n",
       "====================================================================================================\n",
       "Input size (MB): 19.27\n",
       "Forward/backward pass size (MB): 3330.99\n",
       "Params size (MB): 232.27\n",
       "Estimated Total Size (MB): 3582.53\n",
       "===================================================================================================="
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "\n",
    "# Print a summary using torchinfo (uncomment for actual output)\n",
    "input_size = (32, 3, 224, 224)\n",
    "summary(model=model, \n",
    "        input_size= input_size, \n",
    "        col_names=[\"input_size\", \"output_size\", \"num_params\", \"trainable\"],\n",
    "        col_width=10,\n",
    "        row_settings=[\"var_names\"]\n",
    ") \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78733d40",
   "metadata": {},
   "source": [
    "\n",
    "# Getting a pretrained model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "04f3788f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "VisionTransformer(\n",
       "  (conv_proj): Conv2d(3, 768, kernel_size=(16, 16), stride=(16, 16))\n",
       "  (encoder): Encoder(\n",
       "    (dropout): Dropout(p=0.0, inplace=False)\n",
       "    (layers): Sequential(\n",
       "      (encoder_layer_0): EncoderBlock(\n",
       "        (ln_1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "        (self_attention): MultiheadAttention(\n",
       "          (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
       "        )\n",
       "        (dropout): Dropout(p=0.0, inplace=False)\n",
       "        (ln_2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "        (mlp): MLPBlock(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU(approximate='none')\n",
       "          (2): Dropout(p=0.0, inplace=False)\n",
       "          (3): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (4): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (encoder_layer_1): EncoderBlock(\n",
       "        (ln_1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "        (self_attention): MultiheadAttention(\n",
       "          (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
       "        )\n",
       "        (dropout): Dropout(p=0.0, inplace=False)\n",
       "        (ln_2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "        (mlp): MLPBlock(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU(approximate='none')\n",
       "          (2): Dropout(p=0.0, inplace=False)\n",
       "          (3): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (4): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (encoder_layer_2): EncoderBlock(\n",
       "        (ln_1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "        (self_attention): MultiheadAttention(\n",
       "          (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
       "        )\n",
       "        (dropout): Dropout(p=0.0, inplace=False)\n",
       "        (ln_2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "        (mlp): MLPBlock(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU(approximate='none')\n",
       "          (2): Dropout(p=0.0, inplace=False)\n",
       "          (3): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (4): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (encoder_layer_3): EncoderBlock(\n",
       "        (ln_1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "        (self_attention): MultiheadAttention(\n",
       "          (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
       "        )\n",
       "        (dropout): Dropout(p=0.0, inplace=False)\n",
       "        (ln_2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "        (mlp): MLPBlock(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU(approximate='none')\n",
       "          (2): Dropout(p=0.0, inplace=False)\n",
       "          (3): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (4): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (encoder_layer_4): EncoderBlock(\n",
       "        (ln_1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "        (self_attention): MultiheadAttention(\n",
       "          (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
       "        )\n",
       "        (dropout): Dropout(p=0.0, inplace=False)\n",
       "        (ln_2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "        (mlp): MLPBlock(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU(approximate='none')\n",
       "          (2): Dropout(p=0.0, inplace=False)\n",
       "          (3): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (4): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (encoder_layer_5): EncoderBlock(\n",
       "        (ln_1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "        (self_attention): MultiheadAttention(\n",
       "          (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
       "        )\n",
       "        (dropout): Dropout(p=0.0, inplace=False)\n",
       "        (ln_2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "        (mlp): MLPBlock(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU(approximate='none')\n",
       "          (2): Dropout(p=0.0, inplace=False)\n",
       "          (3): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (4): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (encoder_layer_6): EncoderBlock(\n",
       "        (ln_1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "        (self_attention): MultiheadAttention(\n",
       "          (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
       "        )\n",
       "        (dropout): Dropout(p=0.0, inplace=False)\n",
       "        (ln_2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "        (mlp): MLPBlock(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU(approximate='none')\n",
       "          (2): Dropout(p=0.0, inplace=False)\n",
       "          (3): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (4): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (encoder_layer_7): EncoderBlock(\n",
       "        (ln_1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "        (self_attention): MultiheadAttention(\n",
       "          (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
       "        )\n",
       "        (dropout): Dropout(p=0.0, inplace=False)\n",
       "        (ln_2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "        (mlp): MLPBlock(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU(approximate='none')\n",
       "          (2): Dropout(p=0.0, inplace=False)\n",
       "          (3): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (4): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (encoder_layer_8): EncoderBlock(\n",
       "        (ln_1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "        (self_attention): MultiheadAttention(\n",
       "          (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
       "        )\n",
       "        (dropout): Dropout(p=0.0, inplace=False)\n",
       "        (ln_2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "        (mlp): MLPBlock(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU(approximate='none')\n",
       "          (2): Dropout(p=0.0, inplace=False)\n",
       "          (3): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (4): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (encoder_layer_9): EncoderBlock(\n",
       "        (ln_1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "        (self_attention): MultiheadAttention(\n",
       "          (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
       "        )\n",
       "        (dropout): Dropout(p=0.0, inplace=False)\n",
       "        (ln_2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "        (mlp): MLPBlock(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU(approximate='none')\n",
       "          (2): Dropout(p=0.0, inplace=False)\n",
       "          (3): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (4): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (encoder_layer_10): EncoderBlock(\n",
       "        (ln_1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "        (self_attention): MultiheadAttention(\n",
       "          (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
       "        )\n",
       "        (dropout): Dropout(p=0.0, inplace=False)\n",
       "        (ln_2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "        (mlp): MLPBlock(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU(approximate='none')\n",
       "          (2): Dropout(p=0.0, inplace=False)\n",
       "          (3): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (4): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (encoder_layer_11): EncoderBlock(\n",
       "        (ln_1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "        (self_attention): MultiheadAttention(\n",
       "          (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n",
       "        )\n",
       "        (dropout): Dropout(p=0.0, inplace=False)\n",
       "        (ln_2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "        (mlp): MLPBlock(\n",
       "          (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (1): GELU(approximate='none')\n",
       "          (2): Dropout(p=0.0, inplace=False)\n",
       "          (3): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (4): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (ln): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "  )\n",
       "  (heads): Sequential(\n",
       "    (head): Linear(in_features=768, out_features=1000, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "\n",
    "# NEW: Setup the model with pretrained weights and send it to the target device (torchvision v0.13+)\n",
    "weights = torchvision.models.ViT_B_16_Weights.IMAGENET1K_V1\n",
    "model = torchvision.models.vit_b_16(weights = weights, progress = False).to(device)\n",
    "\n",
    "model\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "b3643f2f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "====================================================================================================\n",
       "Layer (type (var_name))                                      Input Shape Output Shape Param #    Trainable\n",
       "====================================================================================================\n",
       "VisionTransformer (VisionTransformer)                        [32, 3, 224, 224] [32, 1000] 768        True\n",
       "├─Conv2d (conv_proj)                                         [32, 3, 224, 224] [32, 768, 14, 14] 590,592    True\n",
       "├─Encoder (encoder)                                          [32, 197, 768] [32, 197, 768] 151,296    True\n",
       "│    └─Dropout (dropout)                                     [32, 197, 768] [32, 197, 768] --         --\n",
       "│    └─Sequential (layers)                                   [32, 197, 768] [32, 197, 768] --         True\n",
       "│    │    └─EncoderBlock (encoder_layer_0)                   [32, 197, 768] [32, 197, 768] 7,087,872  True\n",
       "│    │    └─EncoderBlock (encoder_layer_1)                   [32, 197, 768] [32, 197, 768] 7,087,872  True\n",
       "│    │    └─EncoderBlock (encoder_layer_2)                   [32, 197, 768] [32, 197, 768] 7,087,872  True\n",
       "│    │    └─EncoderBlock (encoder_layer_3)                   [32, 197, 768] [32, 197, 768] 7,087,872  True\n",
       "│    │    └─EncoderBlock (encoder_layer_4)                   [32, 197, 768] [32, 197, 768] 7,087,872  True\n",
       "│    │    └─EncoderBlock (encoder_layer_5)                   [32, 197, 768] [32, 197, 768] 7,087,872  True\n",
       "│    │    └─EncoderBlock (encoder_layer_6)                   [32, 197, 768] [32, 197, 768] 7,087,872  True\n",
       "│    │    └─EncoderBlock (encoder_layer_7)                   [32, 197, 768] [32, 197, 768] 7,087,872  True\n",
       "│    │    └─EncoderBlock (encoder_layer_8)                   [32, 197, 768] [32, 197, 768] 7,087,872  True\n",
       "│    │    └─EncoderBlock (encoder_layer_9)                   [32, 197, 768] [32, 197, 768] 7,087,872  True\n",
       "│    │    └─EncoderBlock (encoder_layer_10)                  [32, 197, 768] [32, 197, 768] 7,087,872  True\n",
       "│    │    └─EncoderBlock (encoder_layer_11)                  [32, 197, 768] [32, 197, 768] 7,087,872  True\n",
       "│    └─LayerNorm (ln)                                        [32, 197, 768] [32, 197, 768] 1,536      True\n",
       "├─Sequential (heads)                                         [32, 768]  [32, 1000] --         True\n",
       "│    └─Linear (head)                                         [32, 768]  [32, 1000] 769,000    True\n",
       "====================================================================================================\n",
       "Total params: 86,567,656\n",
       "Trainable params: 86,567,656\n",
       "Non-trainable params: 0\n",
       "Total mult-adds (Units.GIGABYTES): 5.54\n",
       "====================================================================================================\n",
       "Input size (MB): 19.27\n",
       "Forward/backward pass size (MB): 3330.99\n",
       "Params size (MB): 232.27\n",
       "Estimated Total Size (MB): 3582.53\n",
       "===================================================================================================="
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "\n",
    "# Print a summary using torchinfo (uncomment for actual output)\n",
    "input_size = (32, 3, 224, 224)\n",
    "summary(model=model, \n",
    "        input_size= input_size, \n",
    "        col_names=[\"input_size\", \"output_size\", \"num_params\", \"trainable\"],\n",
    "        col_width=10,\n",
    "        row_settings=[\"var_names\"]\n",
    ") \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "7d0c7235",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Freeze all base layers in the \"features\" section of the model (the feature extractor) by setting requires_grad=False\n",
    "for param in model.encoder.parameters():\n",
    "    param.requires_grad = False\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "b3441926",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the manual seeds\n",
    "torch.manual_seed(42)\n",
    "torch.cuda.manual_seed(42)\n",
    "\n",
    "# Get the length of class_names (one output unit for each class)\n",
    "output_shape = len(class_names)\n",
    "\n",
    "# Recreate the classifier layer and seed it to the target device\n",
    "model.heads = torch.nn.Sequential(\n",
    "    torch.nn.Linear(in_features=768, \n",
    "                    out_features=output_shape, # same number of output units as our number of classes\n",
    "                    bias=True)).to(device)\n",
    "\n",
    "#Change output classes to Astronomy classes\n",
    "weights.meta['categories'] = class_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "357458e1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "====================================================================================================\n",
       "Layer (type (var_name))                                      Input Shape Output Shape Param #    Trainable\n",
       "====================================================================================================\n",
       "VisionTransformer (VisionTransformer)                        [32, 3, 224, 224] [32, 10]   768        Partial\n",
       "├─Conv2d (conv_proj)                                         [32, 3, 224, 224] [32, 768, 14, 14] 590,592    True\n",
       "├─Encoder (encoder)                                          [32, 197, 768] [32, 197, 768] 151,296    False\n",
       "│    └─Dropout (dropout)                                     [32, 197, 768] [32, 197, 768] --         --\n",
       "│    └─Sequential (layers)                                   [32, 197, 768] [32, 197, 768] --         False\n",
       "│    │    └─EncoderBlock (encoder_layer_0)                   [32, 197, 768] [32, 197, 768] (7,087,872) False\n",
       "│    │    └─EncoderBlock (encoder_layer_1)                   [32, 197, 768] [32, 197, 768] (7,087,872) False\n",
       "│    │    └─EncoderBlock (encoder_layer_2)                   [32, 197, 768] [32, 197, 768] (7,087,872) False\n",
       "│    │    └─EncoderBlock (encoder_layer_3)                   [32, 197, 768] [32, 197, 768] (7,087,872) False\n",
       "│    │    └─EncoderBlock (encoder_layer_4)                   [32, 197, 768] [32, 197, 768] (7,087,872) False\n",
       "│    │    └─EncoderBlock (encoder_layer_5)                   [32, 197, 768] [32, 197, 768] (7,087,872) False\n",
       "│    │    └─EncoderBlock (encoder_layer_6)                   [32, 197, 768] [32, 197, 768] (7,087,872) False\n",
       "│    │    └─EncoderBlock (encoder_layer_7)                   [32, 197, 768] [32, 197, 768] (7,087,872) False\n",
       "│    │    └─EncoderBlock (encoder_layer_8)                   [32, 197, 768] [32, 197, 768] (7,087,872) False\n",
       "│    │    └─EncoderBlock (encoder_layer_9)                   [32, 197, 768] [32, 197, 768] (7,087,872) False\n",
       "│    │    └─EncoderBlock (encoder_layer_10)                  [32, 197, 768] [32, 197, 768] (7,087,872) False\n",
       "│    │    └─EncoderBlock (encoder_layer_11)                  [32, 197, 768] [32, 197, 768] (7,087,872) False\n",
       "│    └─LayerNorm (ln)                                        [32, 197, 768] [32, 197, 768] (1,536)    False\n",
       "├─Sequential (heads)                                         [32, 768]  [32, 10]   --         True\n",
       "│    └─Linear (0)                                            [32, 768]  [32, 10]   7,690      True\n",
       "====================================================================================================\n",
       "Total params: 85,806,346\n",
       "Trainable params: 599,050\n",
       "Non-trainable params: 85,207,296\n",
       "Total mult-adds (Units.GIGABYTES): 5.52\n",
       "====================================================================================================\n",
       "Input size (MB): 19.27\n",
       "Forward/backward pass size (MB): 3330.74\n",
       "Params size (MB): 229.22\n",
       "Estimated Total Size (MB): 3579.23\n",
       "===================================================================================================="
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "\n",
    "# # Do a summary *after* freezing the features and changing the output classifier layer (uncomment for actual output)\n",
    "summary(model, \n",
    "        input_size= input_size, # make sure this is \"input_size\", not \"input_shape\" (batch_size, color_channels, height, width)\n",
    "        verbose=0,\n",
    "        col_names=[\"input_size\", \"output_size\", \"num_params\", \"trainable\"],\n",
    "        col_width=10,\n",
    "        row_settings=[\"var_names\"]\n",
    "       )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "7ff4cd9e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "===============================================================================================\n",
       "Layer (type:depth-idx)                        Output Shape              Param #\n",
       "===============================================================================================\n",
       "VisionTransformer                             [32, 10]                  768\n",
       "├─Conv2d: 1-1                                 [32, 768, 14, 14]         590,592\n",
       "├─Encoder: 1-2                                [32, 197, 768]            151,296\n",
       "│    └─Dropout: 2-1                           [32, 197, 768]            --\n",
       "│    └─Sequential: 2-2                        [32, 197, 768]            --\n",
       "│    │    └─EncoderBlock: 3-1                 [32, 197, 768]            (7,087,872)\n",
       "│    │    └─EncoderBlock: 3-2                 [32, 197, 768]            (7,087,872)\n",
       "│    │    └─EncoderBlock: 3-3                 [32, 197, 768]            (7,087,872)\n",
       "│    │    └─EncoderBlock: 3-4                 [32, 197, 768]            (7,087,872)\n",
       "│    │    └─EncoderBlock: 3-5                 [32, 197, 768]            (7,087,872)\n",
       "│    │    └─EncoderBlock: 3-6                 [32, 197, 768]            (7,087,872)\n",
       "│    │    └─EncoderBlock: 3-7                 [32, 197, 768]            (7,087,872)\n",
       "│    │    └─EncoderBlock: 3-8                 [32, 197, 768]            (7,087,872)\n",
       "│    │    └─EncoderBlock: 3-9                 [32, 197, 768]            (7,087,872)\n",
       "│    │    └─EncoderBlock: 3-10                [32, 197, 768]            (7,087,872)\n",
       "│    │    └─EncoderBlock: 3-11                [32, 197, 768]            (7,087,872)\n",
       "│    │    └─EncoderBlock: 3-12                [32, 197, 768]            (7,087,872)\n",
       "│    └─LayerNorm: 2-3                         [32, 197, 768]            (1,536)\n",
       "├─Sequential: 1-3                             [32, 10]                  --\n",
       "│    └─Linear: 2-4                            [32, 10]                  7,690\n",
       "===============================================================================================\n",
       "Total params: 85,806,346\n",
       "Trainable params: 599,050\n",
       "Non-trainable params: 85,207,296\n",
       "Total mult-adds (Units.GIGABYTES): 5.52\n",
       "===============================================================================================\n",
       "Input size (MB): 19.27\n",
       "Forward/backward pass size (MB): 3330.74\n",
       "Params size (MB): 229.22\n",
       "Estimated Total Size (MB): 3579.23\n",
       "==============================================================================================="
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "\n",
    "import torchinfo \n",
    "torchinfo.summary(model, input_size= input_size)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0eea4c9",
   "metadata": {},
   "source": [
    "# Train model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "9f855f5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Define loss and optimizer\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=0.0001)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97ab440d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2d3c21603b5144f588806b13d972e58a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1 | train_loss: 1.4752 | train_acc: 0.4793 | test_loss: 1.2071 | test_acc: 0.5640 | min_test_loss: 1.2071 | \n",
      "Epoch: 2 | train_loss: 1.1332 | train_acc: 0.6027 | test_loss: 1.1067 | test_acc: 0.5972 | min_test_loss: 1.1067 | \n",
      "Epoch: 3 | train_loss: 1.0034 | train_acc: 0.6517 | test_loss: 0.9753 | test_acc: 0.6573 | min_test_loss: 0.9753 | \n",
      "Epoch: 4 | train_loss: 0.9291 | train_acc: 0.6766 | test_loss: 0.8986 | test_acc: 0.6857 | min_test_loss: 0.8986 | \n",
      "Epoch: 5 | train_loss: 0.8698 | train_acc: 0.6994 | test_loss: 0.8951 | test_acc: 0.6924 | min_test_loss: 0.8951 | \n",
      "Epoch: 6 | train_loss: 0.8281 | train_acc: 0.7124 | test_loss: 0.8596 | test_acc: 0.6938 | min_test_loss: 0.8596 | \n",
      "Epoch: 7 | train_loss: 0.8043 | train_acc: 0.7218 | test_loss: 0.8427 | test_acc: 0.7004 | min_test_loss: 0.8427 | \n",
      "Epoch: 8 | train_loss: 0.7749 | train_acc: 0.7336 | test_loss: 0.8532 | test_acc: 0.6964 | min_test_loss: 0.8427 | \n",
      "Epoch: 9 | train_loss: 0.7444 | train_acc: 0.7405 | test_loss: 0.8358 | test_acc: 0.7023 | min_test_loss: 0.8358 | \n",
      "Epoch: 10 | train_loss: 0.7265 | train_acc: 0.7512 | test_loss: 0.7920 | test_acc: 0.7189 | min_test_loss: 0.7920 | \n",
      "Epoch: 11 | train_loss: 0.7193 | train_acc: 0.7503 | test_loss: 0.8092 | test_acc: 0.7141 | min_test_loss: 0.7920 | \n",
      "Epoch: 12 | train_loss: 0.6992 | train_acc: 0.7629 | test_loss: 0.8058 | test_acc: 0.7232 | min_test_loss: 0.7920 | \n",
      "Epoch: 13 | train_loss: 0.6820 | train_acc: 0.7632 | test_loss: 0.7912 | test_acc: 0.7268 | min_test_loss: 0.7912 | \n",
      "Epoch: 14 | train_loss: 0.6624 | train_acc: 0.7726 | test_loss: 0.8281 | test_acc: 0.7144 | min_test_loss: 0.7912 | \n",
      "Epoch: 15 | train_loss: 0.6533 | train_acc: 0.7742 | test_loss: 0.7647 | test_acc: 0.7421 | min_test_loss: 0.7647 | \n",
      "Epoch: 16 | train_loss: 0.6356 | train_acc: 0.7823 | test_loss: 0.7985 | test_acc: 0.7226 | min_test_loss: 0.7647 | \n",
      "Epoch: 17 | train_loss: 0.6210 | train_acc: 0.7842 | test_loss: 0.7934 | test_acc: 0.7251 | min_test_loss: 0.7647 | \n",
      "Epoch: 18 | train_loss: 0.6102 | train_acc: 0.7914 | test_loss: 0.7954 | test_acc: 0.7307 | min_test_loss: 0.7647 | \n",
      "Epoch: 19 | train_loss: 0.5981 | train_acc: 0.7932 | test_loss: 0.7811 | test_acc: 0.7277 | min_test_loss: 0.7647 | \n",
      "Epoch: 20 | train_loss: 0.6005 | train_acc: 0.7947 | test_loss: 0.7880 | test_acc: 0.7243 | min_test_loss: 0.7647 | \n",
      "Epoch: 21 | train_loss: 0.5843 | train_acc: 0.7987 | test_loss: 0.7744 | test_acc: 0.7330 | min_test_loss: 0.7647 | \n",
      "Epoch: 22 | train_loss: 0.5771 | train_acc: 0.7981 | test_loss: 0.7991 | test_acc: 0.7282 | min_test_loss: 0.7647 | \n",
      "Epoch: 23 | train_loss: 0.5687 | train_acc: 0.8056 | test_loss: 0.7580 | test_acc: 0.7392 | min_test_loss: 0.7580 | \n",
      "Epoch: 24 | train_loss: 0.5565 | train_acc: 0.8084 | test_loss: 0.7982 | test_acc: 0.7297 | min_test_loss: 0.7580 | \n",
      "Epoch: 25 | train_loss: 0.5440 | train_acc: 0.8104 | test_loss: 0.7798 | test_acc: 0.7404 | min_test_loss: 0.7580 | \n",
      "Epoch: 26 | train_loss: 0.5317 | train_acc: 0.8139 | test_loss: 0.8847 | test_acc: 0.6944 | min_test_loss: 0.7580 | \n"
     ]
    }
   ],
   "source": [
    "# Set the random seeds\n",
    "torch.manual_seed(50)\n",
    "torch.cuda.manual_seed(50)\n",
    "\n",
    "# Start the timer\n",
    "from timeit import default_timer as timer \n",
    "start_time = timer()\n",
    "\n",
    "# Setup training and save the results\n",
    "results = engine.train(model=model,\n",
    "                       train_dataloader=train_dataloader,\n",
    "                       test_dataloader=test_dataloader,\n",
    "                       optimizer=optimizer,\n",
    "                       loss_fn=loss_fn,\n",
    "                       epochs=100,\n",
    "                       patience=3,\n",
    "                       delta=0.05,\n",
    "                       device=device)\n",
    "\n",
    "# End the timer and print out how long it took\n",
    "end_time = timer()\n",
    "print(f\"[INFO] Total training time: {(end_time-start_time)/60:.3f} minutes\")\n",
    "print(f\"[INFO] Total training time: {(end_time-start_time)/3600:.3f} hours\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "809277db",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
